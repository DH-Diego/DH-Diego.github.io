# 机器学习复习
1. 核函数
  - 核函数相当于数据点做特征扩展后计算范数，每一对数据点产生一个范数，并最终形成核函数矩阵K，其中特征扩展可以将x扩展至无穷维，K是一个N乘N的矩阵，N为数据点的个数，
  而非扩展函数的维数。
  - 实际上对于有限维扩展函数，K的每一个元素记录了一对向量的内积，对于无限维扩展函数，K的每一个元素记录了一对函数的乘积的积分，其实就是一对函数的卷积。
  - 核函数在众多的机器学习算法当中有重要作用，可以通过内积的方法直接跳过无穷维扩展所带来的特征个数爆炸问题，从而将运算复杂度从O(number_of_feature)降到
  O(number_of_sample)， 当feature数量足够多时，后者在计算效率上具有明显的优势。
  - 当我们拟合机器学习的某个参数w时，w一般由x和y共同决定，当我们用\phi(x)扩展x至无穷维时，w由\phi(x),y共同决定，拓展至无穷维，但是在预测时，
  我们计算w·\phi(x_0)^T,从而形成\phi(x)的内积，将维度降低，其实在这个过程中，我们直接省略了w的计算过程，直接利用\phi(x)·\phi(x_0) = K(x,x_0)和y来计算y_0
2. 基于核函数的高斯过程
  - 一个特别的例子就是利用核函数来计算高斯过程，高斯过程就是认为在每一个输入x_0处，y_0都符合一个高斯分布，这个分布的mean和var将由x_0和训练集中的x,y共同决定，
 。实质上mean和var都是x_0的函数，训练集中的x和y都是这两个函数的参数，mean = f(K(x_0,x)，y) , var = g(K((x_0,x)),我们不关心f和g的具体形式，但是我们知道随着
 x_0的出现，我们通过计算k可以得到y_0的高斯分布，从而进行预测。
 
3. 支持向量机
  - SVM通过计算最大边界分类来得到更鲁棒的结果
  - SVM通过求对偶问题解决更为方便，实际上就是通过引入拉格朗日参数并求偏导数，将要计算的参数转化为拉格朗日参数。
  - 物理意义上，对偶问题就是将要二分类的点集补充为两个convex hull， 然后计算这两个convex hull的最近距离，并将实质上的分割线置于过convex hulls
  最近距离连线的中点并且与之垂直的位置上
  - 通过设置slack variable可以解决SVM的软间隔，实际上就是引入\psi_i，for all x_i \in X_train，并将其作为新的参数，加入代价函数中，同样通过转化为对偶问题的方法求解
  - SVM的核函数可以有效的解决低维度下线性不可分的问题。
4. 决策树
  - 决策树主要分为ID3,C4.5,CART三种
  - ID3 通过计算每个属性的最大熵增益来决定每个节点应该按照哪个属性进行分割
  - C4.5解决了ID3 倾向于分割取值多的属性的问题，通过引入属性的熵，并将分割标准从最大熵增益转变为最大熵增益率（利用属性熵来得到）来解决这一问题。
  - CART 有两方面不同，
    - 一是改变了分割指标，改为GINI Index，gini index 物理意义是随机取两个样本，他们的类别不一样的概率
    - 二是改变了树的结构，将多叉树统一为二叉树，每次分割只挑选某个feature的一个子集，并只回答是或否。
    
